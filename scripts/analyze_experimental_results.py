"""
AnÃ¡lisis EstadÃ­stico de Resultados Experimentales
=================================================

Procesa y analiza los resultados de la validaciÃ³n experimental
generando estadÃ­sticas descriptivas y pruebas de significancia.

Autor: Oscar Isaac Laguna Santa Cruz
Universidad: UNMSM - FISI
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import statistics
from datetime import datetime

# Intentar importar librerÃ­as cientÃ­ficas
try:
    import numpy as np
    import pandas as pd
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("âš ï¸ scipy/pandas no disponibles. Usando cÃ¡lculos bÃ¡sicos.")

BASE_DIR = Path(__file__).parent.parent
RESULTS_DIR = BASE_DIR / "data" / "experiments" / "results"


class ExperimentalAnalyzer:
    """Analiza resultados experimentales con mÃ©todos estadÃ­sticos"""
    
    def __init__(self):
        self.results_data = None
        
    def load_latest_results(self) -> Dict:
        """Carga el archivo de resultados mÃ¡s reciente"""
        
        if not RESULTS_DIR.exists():
            print(f"âŒ Directorio de resultados no existe: {RESULTS_DIR}")
            return None
        
        # Buscar archivos de resultados
        result_files = list(RESULTS_DIR.glob("experimental_validation_*.json"))
        
        if not result_files:
            print(f"âŒ No se encontraron archivos de resultados en {RESULTS_DIR}")
            return None
        
        # Ordenar por fecha (mÃ¡s reciente primero)
        latest_file = sorted(result_files, reverse=True)[0]
        
        print(f"ğŸ“‚ Cargando resultados desde: {latest_file.name}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            self.results_data = json.load(f)
        
        return self.results_data
    
    def extract_metrics(self) -> pd.DataFrame:
        """Extrae mÃ©tricas en formato tabular"""
        
        if not self.results_data:
            return None
        
        data_rows = []
        
        for result in self.results_data.get("results", []):
            if "error" in result:
                continue
            
            app_name = result["application"]["name"]
            metrics_comp = result.get("metrics_comparison", {})
            
            # SAST
            if "sast" in metrics_comp:
                data_rows.append({
                    "Application": app_name,
                    "Method": "SAST",
                    "Precision": metrics_comp["sast"]["precision"],
                    "Recall": metrics_comp["sast"]["recall"],
                    "F1-Score": metrics_comp["sast"]["f1_score"],
                    "False_Positives": metrics_comp["sast"]["false_positives"]
                })
            
            # DAST
            if "dast" in metrics_comp:
                data_rows.append({
                    "Application": app_name,
                    "Method": "DAST",
                    "Precision": metrics_comp["dast"]["precision"],
                    "Recall": metrics_comp["dast"]["recall"],
                    "F1-Score": metrics_comp["dast"]["f1_score"],
                    "False_Positives": metrics_comp["dast"]["false_positives"]
                })
            
            # Hybrid
            if "hybrid" in metrics_comp:
                data_rows.append({
                    "Application": app_name,
                    "Method": "HYBRID",
                    "Precision": metrics_comp["hybrid"]["precision"],
                    "Recall": metrics_comp["hybrid"]["recall"],
                    "F1-Score": metrics_comp["hybrid"]["f1_score"],
                    "False_Positives": metrics_comp["hybrid"]["false_positives"]
                })
        
        if SCIPY_AVAILABLE:
            return pd.DataFrame(data_rows)
        else:
            return data_rows
    
    def calculate_descriptive_statistics(self):
        """Calcula estadÃ­sticas descriptivas"""
        
        print("\n" + "="*80)
        print("ğŸ“Š ESTADÃSTICAS DESCRIPTIVAS")
        print("="*80 + "\n")
        
        metrics_data = self.extract_metrics()
        
        if SCIPY_AVAILABLE and isinstance(metrics_data, pd.DataFrame):
            # Usar pandas para anÃ¡lisis
            for method in ["SAST", "DAST", "HYBRID"]:
                method_data = metrics_data[metrics_data["Method"] == method]
                
                if len(method_data) == 0:
                    continue
                
                print(f"\nğŸ” {method}")
                print("-" * 60)
                
                for metric in ["Precision", "Recall", "F1-Score", "False_Positives"]:
                    values = method_data[metric].values
                    
                    print(f"\n{metric}:")
                    print(f"  Media:       {np.mean(values):.4f}")
                    print(f"  Mediana:     {np.median(values):.4f}")
                    print(f"  Desv. Est.:  {np.std(values, ddof=1):.4f}")
                    print(f"  MÃ­n:         {np.min(values):.4f}")
                    print(f"  MÃ¡x:         {np.max(values):.4f}")
                    
                    if len(values) > 1:
                        # Intervalo de confianza 95%
                        ci = stats.t.interval(
                            0.95, 
                            len(values)-1,
                            loc=np.mean(values),
                            scale=stats.sem(values)
                        )
                        print(f"  IC 95%:      [{ci[0]:.4f}, {ci[1]:.4f}]")
        else:
            # AnÃ¡lisis bÃ¡sico sin pandas
            for method in ["SAST", "DAST", "HYBRID"]:
                method_data = [row for row in metrics_data if row["Method"] == method]
                
                if not method_data:
                    continue
                
                print(f"\nğŸ” {method}")
                print("-" * 60)
                
                for metric in ["Precision", "Recall", "F1-Score", "False_Positives"]:
                    values = [row[metric] for row in method_data]
                    
                    if values:
                        print(f"\n{metric}:")
                        print(f"  Media:       {statistics.mean(values):.4f}")
                        print(f"  Mediana:     {statistics.median(values):.4f}")
                        print(f"  Desv. Est.:  {statistics.stdev(values) if len(values) > 1 else 0:.4f}")
                        print(f"  MÃ­n:         {min(values):.4f}")
                        print(f"  MÃ¡x:         {max(values):.4f}")
    
    def perform_hypothesis_testing(self):
        """Realiza pruebas de hipÃ³tesis estadÃ­sticas"""
        
        if not SCIPY_AVAILABLE:
            print("\nâš ï¸ scipy no disponible. Omitiendo pruebas de hipÃ³tesis.")
            return
        
        print("\n" + "="*80)
        print("ğŸ§ª PRUEBAS DE HIPÃ“TESIS")
        print("="*80 + "\n")
        
        metrics_data = self.extract_metrics()
        
        # H0: No hay diferencia significativa entre SAST y HYBRID
        # H1: HYBRID es significativamente mejor que SAST
        
        print("HipÃ³tesis Nula (H0): Î¼_SAST = Î¼_HYBRID")
        print("HipÃ³tesis Alternativa (H1): Î¼_HYBRID > Î¼_SAST")
        print("Nivel de significancia: Î± = 0.05\n")
        
        sast_data = metrics_data[metrics_data["Method"] == "SAST"]
        hybrid_data = metrics_data[metrics_data["Method"] == "HYBRID"]
        
        for metric in ["Precision", "Recall", "F1-Score"]:
            sast_values = sast_data[metric].values
            hybrid_values = hybrid_data[metric].values
            
            if len(sast_values) < 2 or len(hybrid_values) < 2:
                continue
            
            # Prueba t de Student para muestras pareadas
            t_stat, p_value = stats.ttest_rel(hybrid_values, sast_values)
            
            # Cohen's d (tamaÃ±o del efecto)
            pooled_std = np.sqrt((np.std(sast_values, ddof=1)**2 + np.std(hybrid_values, ddof=1)**2) / 2)
            cohens_d = (np.mean(hybrid_values) - np.mean(sast_values)) / pooled_std if pooled_std > 0 else 0
            
            print(f"\nğŸ“ˆ {metric}")
            print("-" * 60)
            print(f"  SAST (Î¼â‚):    {np.mean(sast_values):.4f} Â± {np.std(sast_values, ddof=1):.4f}")
            print(f"  HYBRID (Î¼â‚‚):  {np.mean(hybrid_values):.4f} Â± {np.std(hybrid_values, ddof=1):.4f}")
            print(f"  Diferencia:   {np.mean(hybrid_values) - np.mean(sast_values):.4f}")
            print(f"  t-statistic:  {t_stat:.4f}")
            print(f"  p-value:      {p_value:.4f}")
            print(f"  Cohen's d:    {cohens_d:.4f}")
            
            # InterpretaciÃ³n
            if p_value < 0.05:
                print(f"  âœ… Resultado: RECHAZAMOS H0 (p < 0.05)")
                print(f"     â†’ HYBRID es significativamente mejor que SAST en {metric}")
            else:
                print(f"  âŒ Resultado: NO RECHAZAMOS H0 (p â‰¥ 0.05)")
                print(f"     â†’ No hay evidencia suficiente de diferencia significativa")
            
            # TamaÃ±o del efecto
            if abs(cohens_d) < 0.2:
                effect = "PEQUEÃ‘O"
            elif abs(cohens_d) < 0.5:
                effect = "MEDIANO"
            elif abs(cohens_d) < 0.8:
                effect = "GRANDE"
            else:
                effect = "MUY GRANDE"
            
            print(f"  TamaÃ±o del efecto: {effect}")
    
    def analyze_false_positive_reduction(self):
        """Analiza la reducciÃ³n de falsos positivos"""
        
        print("\n" + "="*80)
        print("ğŸ¯ ANÃLISIS DE REDUCCIÃ“N DE FALSOS POSITIVOS")
        print("="*80 + "\n")
        
        reductions = []
        
        for result in self.results_data.get("results", []):
            if "error" in result or "false_positive_reduction" not in result:
                continue
            
            fp_reduction = result["false_positive_reduction"]
            app_name = result["application"]["name"]
            
            print(f"\nğŸ“¦ {app_name}")
            print(f"  SAST FP:      {fp_reduction['sast_fp']}")
            print(f"  Hybrid FP:    {fp_reduction['hybrid_fp']}")
            print(f"  ReducciÃ³n:    {fp_reduction['absolute']} ({fp_reduction['percentage']:.1f}%)")
            
            reductions.append(fp_reduction['percentage'])
        
        if reductions:
            print(f"\nğŸ“Š RESUMEN AGREGADO")
            print("-" * 60)
            print(f"  ReducciÃ³n promedio:  {statistics.mean(reductions):.2f}%")
            print(f"  ReducciÃ³n mediana:   {statistics.median(reductions):.2f}%")
            print(f"  ReducciÃ³n mÃ­nima:    {min(reductions):.2f}%")
            print(f"  ReducciÃ³n mÃ¡xima:    {max(reductions):.2f}%")
            
            if len(reductions) > 1:
                print(f"  DesviaciÃ³n estÃ¡ndar: {statistics.stdev(reductions):.2f}%")
    
    def generate_latex_table(self):
        """Genera tabla en formato LaTeX para tesis"""
        
        print("\n" + "="*80)
        print("ğŸ“„ TABLA LATEX PARA TESIS")
        print("="*80 + "\n")
        
        metrics_data = self.extract_metrics()
        
        if not SCIPY_AVAILABLE:
            print("âš ï¸ pandas no disponible. No se puede generar tabla LaTeX.")
            return
        
        # Tabla de mÃ©tricas por mÃ©todo
        latex_table = """
\\begin{table}[htbp]
\\centering
\\caption{ComparaciÃ³n de mÃ©tricas entre mÃ©todos de anÃ¡lisis}
\\label{tab:metrics_comparison}
\\begin{tabular}{lcccc}
\\toprule
\\textbf{MÃ©todo} & \\textbf{PrecisiÃ³n} & \\textbf{Recall} & \\textbf{F1-Score} & \\textbf{FP Promedio} \\\\
\\midrule
"""
        
        for method in ["SAST", "DAST", "HYBRID"]:
            method_data = metrics_data[metrics_data["Method"] == method]
            
            if len(method_data) == 0:
                continue
            
            precision = f"{method_data['Precision'].mean():.3f}"
            recall = f"{method_data['Recall'].mean():.3f}"
            f1 = f"{method_data['F1-Score'].mean():.3f}"
            fp = f"{method_data['False_Positives'].mean():.1f}"
            
            latex_table += f"{method} & {precision} & {recall} & {f1} & {fp} \\\\\n"
        
        latex_table += """\\bottomrule
\\end{tabular}
\\end{table}
"""
        
        print(latex_table)
        
        # Guardar en archivo
        output_file = RESULTS_DIR / "metrics_table.tex"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(latex_table)
        
        print(f"\nâœ… Tabla guardada en: {output_file}")
    
    def run_full_analysis(self):
        """Ejecuta anÃ¡lisis completo"""
        
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘     ANÃLISIS ESTADÃSTICO - ValidaciÃ³n Experimental                  â•‘
â•‘     HybridSecScan                                                    â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # Cargar datos
        if not self.load_latest_results():
            print("âŒ No se pudieron cargar resultados")
            return 1
        
        # AnÃ¡lisis
        try:
            self.calculate_descriptive_statistics()
            self.perform_hypothesis_testing()
            self.analyze_false_positive_reduction()
            self.generate_latex_table()
            
            print("\n" + "="*80)
            print("âœ… ANÃLISIS COMPLETADO")
            print("="*80 + "\n")
            
            return 0
            
        except Exception as e:
            print(f"\nâŒ Error en anÃ¡lisis: {str(e)}")
            import traceback
            traceback.print_exc()
            return 1


def main():
    analyzer = ExperimentalAnalyzer()
    return analyzer.run_full_analysis()


if __name__ == "__main__":
    sys.exit(main())
